import pandas as pd
import os
import pydicom
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG16

# Función para cargar los datos
def load_data(train_csv_path, train_dir, new_size):
    train_df = pd.read_csv(train_csv_path)
    images = []
    labels = []
    for index, row in train_df.iterrows():
        image_path = os.path.join(train_dir, row['SOPInstanceUID'] + '.dcm')
        image = pydicom.dcmread(image_path).pixel_array
        resized_image = cv2.resize(image, new_size)
        # Convertir la imagen a formato RGB
        resized_image_rgb = cv2.cvtColor(resized_image, cv2.COLOR_GRAY2RGB)
        images.append(resized_image_rgb)
        labels.append(row['Target'])
    return np.array(images), np.array(labels)

# Definir el nuevo tamaño de las imágenes
new_size = (256, 256)

# Rutas de los archivos
train_csv_path = 'C:\\Users\\EDEM\\Downloads\\DATAPROJECT4\\train.csv'
train_dir = 'C:\\Users\\EDEM\\Downloads\\DATAPROJECT4\\train'
sample_submission_path = 'C:\\Users\\EDEM\\Downloads\\DATAPROJECT4\\sample_submission.csv'
output_csv_path = 'C:\\Users\\EDEM\\Downloads\\DATAPROJECT4\\intentorelunoche.csv'

#CELDA 2>
# Cargar datos de entrenamiento
train_images, train_labels = load_data(train_csv_path, train_dir, new_size)

# Dividir los datos en conjuntos de entrenamiento y validación
train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)

# Normalizar las imágenes y convertir las etiquetas a one-hot encoding
train_images = train_images / 255.0
val_images = val_images / 255.0
num_classes = len(np.unique(train_labels))
train_labels = to_categorical(train_labels, num_classes=num_classes)
val_labels = to_categorical(val_labels, num_classes=num_classes)

# Guardar los datos de entrenamiento y validación como archivos .npy
np.save('train_images.npy', train_images)
np.save('train_labels.npy', train_labels)
np.save('val_images.npy', val_images)
np.save('val_labels.npy', val_labels)
#TARDA +- 50 MINS

#CELDA 3>
# Definir y compilar el modelo
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))

for layer in base_model.layers:
    layer.trainable = False

model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compilar el modelo
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

#CELDA 4>
# Entrenar el modelo
model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=20, batch_size=64)

#CELDA 5>
# Generar predicciones para el conjunto de prueba
test_images, _ = load_data(sample_submission_path, 'C:\\Users\\EDEM\\Downloads\\DATAPROJECT4\\test', new_size)
test_images = test_images / 255.0
predictions = model.predict(test_images)
predicted_labels = np.argmax(predictions, axis=1)

# Crear un DataFrame con las predicciones
submission_df = pd.DataFrame({'SOPInstanceUID': pd.read_csv(sample_submission_path)['SOPInstanceUID'], 'Target': predicted_labels})

# Guardar el DataFrame como un archivo CSV
submission_df.to_csv(output_csv_path, index=False)
#+- 15 MINS
